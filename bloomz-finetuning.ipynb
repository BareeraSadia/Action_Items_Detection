{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-16T07:48:24.126374Z",
     "iopub.status.busy": "2024-05-16T07:48:24.126019Z",
     "iopub.status.idle": "2024-05-16T07:48:24.776581Z",
     "shell.execute_reply": "2024-05-16T07:48:24.775613Z",
     "shell.execute_reply.started": "2024-05-16T07:48:24.126343Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# load portuguese legal QnA datasetL LegalQA\n",
    "dataset = pd.read_csv(\"/kaggle/input/balanced-5-8k/balanced_AI_5.8k.csv\")\n",
    "\n",
    "def buildprompt(data):\n",
    "    prompt = {}\n",
    "    prompt['text'] = \"Given the question delimited by triple backticks  ```{\" + data['question'] + \"}```, what is the answer?  Answer: {\" + data['answer'] + \"}\"    \n",
    "    return prompt\n",
    "\n",
    "dataset['prompt'] = dataset.apply(buildprompt, axis=1)\n",
    "\n",
    "result = dataset['prompt'].to_list()\n",
    "# save prompts to a json file\n",
    "with open('prompts.json', 'w') as outfile:\n",
    "    json.dump(result, outfile, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:48:24.778605Z",
     "iopub.status.busy": "2024-05-16T07:48:24.778316Z",
     "iopub.status.idle": "2024-05-16T07:48:24.795895Z",
     "shell.execute_reply": "2024-05-16T07:48:24.795049Z",
     "shell.execute_reply.started": "2024-05-16T07:48:24.778580Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'Given the question delimited by triple backticks  ```{مریم آپ ان بلز کی کاپی کروا لینا یاد سے}```, what is the answer?  Answer: {ہاں}'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Open and read the JSON file\n",
    "with open('/kaggle/working/prompts.json', 'r') as infile:\n",
    "    prompts_data = json.load(infile)\n",
    "\n",
    "# Display the contents of the JSON file\n",
    "print(prompts_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:48:24.797398Z",
     "iopub.status.busy": "2024-05-16T07:48:24.797128Z",
     "iopub.status.idle": "2024-05-16T07:48:42.059633Z",
     "shell.execute_reply": "2024-05-16T07:48:42.058807Z",
     "shell.execute_reply.started": "2024-05-16T07:48:24.797375Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 07:48:33.544238: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-16 07:48:33.544343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-16 07:48:33.672452: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from transformers import BloomTokenizerFast, BloomForCausalLM, TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:48:42.061397Z",
     "iopub.status.busy": "2024-05-16T07:48:42.060821Z",
     "iopub.status.idle": "2024-05-16T07:49:45.079005Z",
     "shell.execute_reply": "2024-05-16T07:49:45.078053Z",
     "shell.execute_reply.started": "2024-05-16T07:48:42.061371Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e71e83944b5c44f6a03d60a145e65f06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea03c1c1c5894e3791933f6c9fd07ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40012a5df465423599766757ef9619de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bebdc65a0fa46cdaf5cc04903fd4250",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d7b8a1184845f59e0554038921f49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Loading bloomz model and tokenizer \n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloomz-1b1\")\n",
    "model = BloomForCausalLM.from_pretrained(\"bigscience/bloomz-1b1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:49:45.082175Z",
     "iopub.status.busy": "2024-05-16T07:49:45.081854Z",
     "iopub.status.idle": "2024-05-16T07:49:45.343372Z",
     "shell.execute_reply": "2024-05-16T07:49:45.342492Z",
     "shell.execute_reply.started": "2024-05-16T07:49:45.082148Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93fba1b4c57742a695beda4dc7a8e90a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"prompts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:49:45.344761Z",
     "iopub.status.busy": "2024-05-16T07:49:45.344462Z",
     "iopub.status.idle": "2024-05-16T07:49:48.912473Z",
     "shell.execute_reply": "2024-05-16T07:49:48.911422Z",
     "shell.execute_reply.started": "2024-05-16T07:49:45.344736Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5830\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:49:48.914062Z",
     "iopub.status.busy": "2024-05-16T07:49:48.913736Z",
     "iopub.status.idle": "2024-05-16T07:49:48.920499Z",
     "shell.execute_reply": "2024-05-16T07:49:48.919580Z",
     "shell.execute_reply.started": "2024-05-16T07:49:48.914036Z"
    }
   },
   "outputs": [],
   "source": [
    "# prepare the data for training\n",
    "def prepare_train_data(data):\n",
    "    text_input = data['text']\n",
    "    tokenized_input = tokenizer(text_input, return_tensors='pt', padding=True, truncation=True, max_length=200)\n",
    "    # Ensure labels are tensors of shape (batch_size, sequence_length)\n",
    "    tokenized_input['labels'] = tokenized_input['input_ids'].clone()\n",
    "    return tokenized_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:49:48.921983Z",
     "iopub.status.busy": "2024-05-16T07:49:48.921677Z",
     "iopub.status.idle": "2024-05-16T07:49:50.358580Z",
     "shell.execute_reply": "2024-05-16T07:49:50.357701Z",
     "shell.execute_reply.started": "2024-05-16T07:49:48.921958Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c940423be214d4681533a1885dfa906",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5830 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = dataset['train'].map(prepare_train_data, batched=True, remove_columns=[\"text\"], batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:49:50.360094Z",
     "iopub.status.busy": "2024-05-16T07:49:50.359808Z",
     "iopub.status.idle": "2024-05-16T07:49:50.369841Z",
     "shell.execute_reply": "2024-05-16T07:49:50.367696Z",
     "shell.execute_reply.started": "2024-05-16T07:49:50.360070Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input sequences:\n",
      "106\n",
      "106\n",
      "106\n",
      "106\n",
      "106\n"
     ]
    }
   ],
   "source": [
    "# Inspect the data\n",
    "print(\"Length of input sequences:\")\n",
    "for i in range(5):  # Print information for the first 5 samples\n",
    "    sample = train_dataset[i]\n",
    "    print(len(sample['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:49:50.371284Z",
     "iopub.status.busy": "2024-05-16T07:49:50.371006Z",
     "iopub.status.idle": "2024-05-16T07:49:50.393228Z",
     "shell.execute_reply": "2024-05-16T07:49:50.392282Z",
     "shell.execute_reply.started": "2024-05-16T07:49:50.371260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized text: {'input_ids': [208519, 4264, 4946, 5870, 18488, 23923, 3415, 9374, 23671, 1202, 28703, 63471, 31011, 1202, 35559, 2045, 47049, 9374, 2174, 41585, 2945, 10401, 2379, 129285, 1466, 169321, 1468, 2563, 7977, 7235, 2379, 105694, 18848, 117515, 1468, 10195, 14277, 63471, 31011, 5353, 3415, 6560, 152169, 25229, 18867, 12473], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Check tokenization\n",
    "sample_text = \"اچھا تو کیا اپ ڈیٹس ہیں آپ لوگوں کے پاس انٹرنز کے حوالے سے کیونکہ آپ کو یاد ہو گا کہ لاسٹ ٹائم میں نے کہا تھا کہ نیکسٹ میٹنگ میں ہم لوگ انٹرنز جو ہیں وہ فائنلائز کریں گے\"\n",
    "tokenized_text = tokenizer(sample_text)\n",
    "print(\"Tokenized text:\", tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:49:50.394937Z",
     "iopub.status.busy": "2024-05-16T07:49:50.394590Z",
     "iopub.status.idle": "2024-05-16T07:49:50.511564Z",
     "shell.execute_reply": "2024-05-16T07:49:50.510479Z",
     "shell.execute_reply.started": "2024-05-16T07:49:50.394908Z"
    }
   },
   "outputs": [],
   "source": [
    "# setting arguments to be used during training\n",
    "training_arguments = TrainingArguments(\n",
    "    'AI-bloom-1b1-5.8k',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    optim=\"adafactor\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    save_strategy='no'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:49:50.513256Z",
     "iopub.status.busy": "2024-05-16T07:49:50.512660Z",
     "iopub.status.idle": "2024-05-16T07:49:50.519060Z",
     "shell.execute_reply": "2024-05-16T07:49:50.517579Z",
     "shell.execute_reply.started": "2024-05-16T07:49:50.513223Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "\n",
    "# # Configure the tokenizer for padding and truncation\n",
    "# tokenizer.padding = True\n",
    "# tokenizer.truncation = True\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,  # Train on causal language modeling instead of masked language modeling\n",
    "    pad_to_multiple_of=8  # Set the padding to a multiple of 8 (adjust as needed)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:49:50.520823Z",
     "iopub.status.busy": "2024-05-16T07:49:50.520247Z",
     "iopub.status.idle": "2024-05-16T07:49:53.465633Z",
     "shell.execute_reply": "2024-05-16T07:49:53.464668Z",
     "shell.execute_reply.started": "2024-05-16T07:49:50.520790Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/accelerate/accelerator.py:436: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    data_collator=data_collator,  # Pass the data collator here\n",
    "    train_dataset=train_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T07:49:53.469290Z",
     "iopub.status.busy": "2024-05-16T07:49:53.469003Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"AI_Bloomz1b1_5.8k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.make_archive('finetuned_AI_Bloomz1b1_5.8k', 'zip', '/kaggle/working/AI_Bloomz1b1_5.8k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import FileLink\n",
    "FileLink(r'finetuned_AI_Bloomz1b1_5.8k.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import torch\n",
    "import time\n",
    "from transformers import pipeline, BloomTokenizerFast, BloomForCausalLM\n",
    "\n",
    "# Loading the fine-tuned model: LegalQA-bloom-560m\n",
    "tokenizer = BloomTokenizerFast.from_pretrained(\"bigscience/bloomz-1b1\")\n",
    "model = BloomForCausalLM.from_pretrained(\"/kaggle/working/AI_Bloomz1b1_5.8k\")\n",
    "prompt = 'Given the question delimited by triple backticks ```{{ {} }}```, what is the answer? Answer:'\n",
    "\n",
    "generator = pipeline('text-generation', \n",
    "                     model=model, \n",
    "                     tokenizer=tokenizer,\n",
    "                     do_sample=False)\n",
    "\n",
    "\n",
    "def generate_answers(input_csv_file, output_csv_file):\n",
    "    with open(input_csv_file, 'r', newline='', encoding='utf-8') as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        data = list(reader)\n",
    "\n",
    "    # Open the output CSV file in append mode to keep adding rows\n",
    "    with open(output_csv_file, 'a', newline='', encoding='utf-8') as outfile:\n",
    "        writer = csv.writer(outfile)\n",
    "        \n",
    "        # If the file is empty, write the header\n",
    "        if outfile.tell() == 0:\n",
    "            writer.writerow(['Question', 'Generated Answer'])\n",
    "\n",
    "        for row in data:\n",
    "            question = row['Question']\n",
    "            updated_prompt = prompt.format(question)\n",
    "            result = generator(updated_prompt, max_length=128)\n",
    "            generated_answer = result[0]['generated_text'].replace(updated_prompt, '').strip()\n",
    "            writer.writerow([question, generated_answer])\n",
    "\n",
    "            # Flush the buffer to ensure the data is written to the file immediately\n",
    "            outfile.flush()\n",
    "\n",
    "# Example usage\n",
    "input_csv_file = '/kaggle/input/test-data/updated_Testing_verified_meetings_10.csv'  # Change this to the path of your input CSV file\n",
    "output_csv_file = 'generated_answers_bloomz1b1_5.8k.csv'  # Change this to the path where you want to save the output CSV file\n",
    "\n",
    "while True:\n",
    "    generate_answers(input_csv_file, output_csv_file)\n",
    "    # Wait for some time before processing the next batch of questions\n",
    "    time.sleep(300)  # Wait for 5 minutes before processing the next batch\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 4929045,
     "sourceId": 8297543,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4929758,
     "sourceId": 8298480,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4930445,
     "sourceId": 8299441,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4937194,
     "sourceId": 8311072,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
